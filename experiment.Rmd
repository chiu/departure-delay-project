---
title: "Stat 652 Project Guidelines"
author: "Brad McNeney"
date: '2019-10-16'
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Data 

The data are on flights from three New York City airports 
in 2013, from the `nycflights13` package. 
Data were combined from four datasets from this package:

* `flights`
* `weather`
* `airports`, and
* `planes`

Please read about the variables in each dataset by typing
`help(datasetname)` from the R console. 

```{r}
library(tidyverse)
library(nycflights13)
#help(flights)
#help(weather)
#help(airports)
#help(planes)
fltrain <- read_csv("fltrain.csv.gz")

# when the test data are made available:
# fltest <- read_csv("fltest.csv.gz")
```

Your task is to build a prediction model for 
departure delays. I have held out 1336776 observations
as a test dataset that I will release a few days 
before the project is due.

## Project Length and Scope

Your report should be no more than 5 pages long, plus
references. You must also include an Appendix of R code that
can be used to reproduce the analyses refered to in
the report. There is no page limit for the Appendix, but
please use judgement about what to include. Too long
and it is not likely to be read.
You are encouraged to 
try several prediction methods, and can compare 
these methods, but your report should focus on 
one method in particular. 
You **must** use methods discussed in class.

## Grading Criteria

The criteria for the report are as follows.

### Report (25 marks)

The report should have the following sections

1. Introduction (brief)
2. Data (brief)
3. Methods
4. Results
5. Conclusions and Discussion  



# 1. Introduction (brief)
# 2. Data (brief)
# 3. Methods

## Loading libraries

```{r}
library(tidyverse)
```


## Loading the data
```{r}
library(nycflights13)
set.seed(42)
#help(flights)
#help(weather)
#help(airports)
#help(planes)
fltrain <- read_csv("fltrain.csv.gz")
```

# Preprocessing
dropping the columns "dep_time", "arr_time", "air_time", "arr_delay", because that leaks the response variable. 
dropping column  "year.x" because it contains redundant information 
dropping tailnum because it produces too many dummy variable columns for one hot encoding. 

```{r}
class(fltrain)
fltrain['sched_air_time'] <- fltrain['sched_arr_time']-fltrain['sched_dep_time']
#drops <- c("dep_time", "arr_time", "air_time", "arr_delay", 'sched_arr_time')
drops <- c("dep_time", "arr_time", "air_time", "arr_delay", "year.x", 'tailnum')
fltrain <- fltrain[ , !(names(fltrain) %in% drops)]
fltrain
```

# Plan
# do some data filtering and processing
do normalization of every column first except for intercept and response 
do feature selection with cp estimates or k folds cross validation
try pca?

# do boosted regression, ie gradient boosting or xgboost.

# do hybrid lasso and ridge regression
```{r}
data_df <- fltrain
```


```{r}
nrow(data_df)
```

```{r}
library(dplyr)
non_extreme_delay_subset <- data_df[data_df$dep_delay < 30,]
non_extreme_delay_subset
hist(non_extreme_delay_subset$dep_delay)
nrow(non_extreme_delay_subset)
```

```{r}
data_df <- non_extreme_delay_subset
```


# try features scaling
```{r}
dep_delay_vec <- data_df$dep_delay
data_df$dep_delay <- NULL
head(data_df)
scale(data_df)
head()

```

```{r}
"dep_delay~month+day+sched_dep_time+sched_arr_time+carrier+flight+origin+dest+distance+hour+minute+time_hour+temp+dewp+humid+wind_dir+wind_speed+wind_gust+precip+pressure+visib+name+lat+lon+alt+tz+dst+tzone+year.y+type+manufacturer+model+engines+seats+speed+engine+sched_air_time"
full_model <- lm(dep_delay~month+day+sched_dep_time+sched_arr_time+carrier+flight , data=data_df)
summary <- round(summary(full_model)$coefficients,6)
sorteddf <- summary[order(summary[,ncol(summary)]),]
sorteddf
summary
```
Error in `contrasts<-`(`*tmp*`, value = contr.funs[1 + isOF[nn]]) : contrasts can be applied only to factors with 2 or more levels


```{r}
set.seed(21)
vector <- character(length(colnames(data_df)))
vector <- c(0)

# drop dep_delay from features because it is reponse
# drop tailnum because it produces too many columns 

drops <- c("dep_delay")
drops <- append(drops, 'speed')
drops
```


```{r}
#dropping speed because it has too many NA's.
x_features <- data_df[ , !(names(data_df) %in% drops)]
print(x_features)

full_formula_vec <- c(length(colnames(x_features)))

for (i in 1:length(colnames(x_features))) 
{
  vector[i] <- colnames(x_features)[i]
  formula <- paste0(vector, sep="", collapse="+")
  full_formula <- paste0(c('dep_delay', formula), sep="", collapse="~")
  full_formula_vec[i] <- full_formula
}
```

```{r}
full_formula_vec[length(full_formula_vec)]
```

full_formula_vec up to 34 is ok.
```{r}
length(full_formula_vec)
```

```{r}
full_formula_vec[35]
```

```{r}
x_features[35]
```

```{r}
head(data_df)
```

```{r}
full_model <- lm(full_formula_vec[length(full_formula_vec)] , data=data_df)
summary <- round(summary(full_model)$coefficients,6)
sorted_df <- summary[order(summary[,ncol(summary)]),]
head(sorted_df)
```


```{r}
help(weather)
```


"wind gust is the most important feature according to the linear model.
All gusts are a type of wind. A gust is a sudden increase of the windâ€™s speed that lasts no more than 20 seconds. This usually occurs when wind speeds reach a peak of at least 16 knots. A wind gust usually comes in 2-minute intervals." source - differencebetween.net

 "http://www.differencebetween.net/science/nature/difference-between-gust-and-wind/#ixzz65PVE7Ffb"
 
 
there was something wrong with the speed variable which caused this error involving factors and was therefore dropped.  


#drop tailnum too computationally expensive, adds tooo many columns



```{r}
first_formula <- full_formula_vec[length(full_formula_vec)]
first_formula
```


```{r}
library(gbm)
?gbm
```


```{r}
colnames(data_df)
```


```{r}
relevant_columns <- append(colnames(x_features), "dep_delay")
relevant_columns
relevant_columns <- relevant_columns[!relevant_columns %in% c('time_hour', 'sched_dep_time')]
relevant_columns
```

remove scheduled departure time because it is redundant with hour and minute. 


```{r}
data_df_cols <- colnames(data_df)
data_df_cols
```

```{r}
setdiff(data_df_cols, relevant_columns)
```

# turning all columns with datatype characters to factors. 
```{r}
data_df[sapply(data_df, is.character)] <- lapply(data_df[sapply(data_df, is.character)], 
                                       as.factor)
str(data_df)
```

```{r}
head(data_df)
```

```{r}
View(data_df)
```

```{r}
set.seed(42)
data_df <- data_df[!is.na(data_df$dep_delay), relevant_columns]
hboost <- gbm(dep_delay ~ ., data=data_df,
              n.trees=100, shrinkage=0.01) # default shrinkage = 0.1
```


```{r}
summary(hboost)
```

sched departure time has too much influence, redudant information with hour and time. 

dewpointis important for some reason. 
pressure which is Sea level pressure in millibars is strangely important too. 

```{r}
2^6
```


Cross Validation

```{r}
train <- sample(1:nrow(data_df),size=2*nrow(data_df)/3,replace=FALSE)
train_df <- data_df[train,]
test_df <- data_df[-train,]
```

```{r}
nrow(train_df)
head(train_df)
```

```{r}
nrow(test_df)
head(test_df)
```


```{r}
library(e1071)
#obj <- tune(svm, dep_delay~., data=data_df, 
              #ranges = list(n.trees = 2^(5:10)),
              #tunecontrol = tune.control(sampling = "fix")
             #)
#summary(obj)
#plot(obj)
```



```{r}
mean(abs(test_df$dep_delay))
```

# predicting 0
```{r}
mse = mean((test_df$dep_delay-0)^2)
mse
sqrt(mse)
```

# predicting the mean
```{r}
mse = mean((test_df$dep_delay-mean(train_df$dep_delay))^2)
mse
sqrt(mse)
```


```{r}
mean(train_df$dep_delay)
```


#can try median for this


# using linear regression and imputing with mode and median. 
```{r}
drops <- c('dest', 'name', 'manufacturer', 'model', 'tzone', 'engine', 'sched_air_time')
lm_data_df <- data_df[ , !(names(data_df) %in% drops)]

lm_na_omit_df <- imputeMissings:::impute(lm_data_df)

train <- sample(1:nrow(lm_na_omit_df),size=2*nrow(lm_na_omit_df)/3,replace=FALSE)
lm_train_df <- lm_na_omit_df[train,]
lm_test_df <- lm_na_omit_df[-train,]

model <- lm(dep_delay ~ ., data=lm_train_df)

summary <- round(summary(model)$coefficients,6)
sorteddf <- summary[order(summary[,ncol(summary)]),]
#sorteddf

preds = predict(model, newdata=lm_test_df)
mse = mean((lm_test_df$dep_delay - preds)^2)

mse
sqrt(mse)
#summary(model)
```

# predicting the mean
```{r}
mse = mean((lm_test_df$dep_delay-mean(lm_test_df$dep_delay))^2)
mse
sqrt(mse)
```


# using linear regression and imputing with mode and median. 
```{r}
drops <- c('dest', 'name', 'manufacturer', 'model', 'tzone', 'engine', 'sched_air_time')
lm_data_df <- data_df[ , !(names(data_df) %in% drops)]

lm_na_omit_df <- imputeMissings::impute(lm_data_df, method = 'median/mode', flag=TRUE)

train <- sample(1:nrow(lm_na_omit_df),size=2*nrow(lm_na_omit_df)/3,replace=FALSE)
lm_train_df <- lm_na_omit_df[train,]
lm_test_df <- lm_na_omit_df[-train,]

model <- lm(dep_delay ~ ., data=lm_train_df)

summary <- round(summary(model)$coefficients,6)
sorteddf <- summary[order(summary[,ncol(summary)]),]

preds = predict(model, newdata=lm_test_df)
mse = mean((lm_test_df$dep_delay - preds)^2)

mse
sqrt(mse)
```

# using gbm
```{r}
hboost <- gbm(dep_delay ~ ., data=lm_train_df,
              n.trees=1000, shrinkage=0.01)
```

```{r}
preds = predict(hboost, n.trees = 1000, newdata=lm_test_df)
mse = mean((lm_test_df$dep_delay - preds)^2)
mse
sqrt(mse)
```

# using gbm
```{r}
hboost <- gbm(dep_delay ~ ., data=lm_train_df,
              n.trees=100, shrinkage=0.1)
preds = predict(hboost, n.trees = 100, newdata=lm_test_df)
mse = mean((lm_test_df$dep_delay - preds)^2)
mse
sqrt(mse)
```


```{r}
hboost <- gbm(dep_delay ~ ., data=lm_train_df,
              n.trees=100, distribution='gaussian', shrinkage=0.2)
preds = predict(hboost, n.trees = 100, newdata=lm_test_df)
mse = mean((lm_test_df$dep_delay - preds)^2)
mse
sqrt(mse)
```

# take shrinkage or learning rate to be as small as possible and tune number of trees.

```{r}
hboost <- gbm(dep_delay ~ ., data=lm_train_df,
              n.trees=100, distribution='gaussian', shrinkage=0.2)
preds = predict(hboost, n.trees = 100, newdata=lm_test_df)
mse = mean((lm_test_df$dep_delay - preds)^2)
mse
sqrt(mse)
```


```{r}
set.seed(42)

x <- 2^seq(5,14, by=1)
mse_vec <- numeric(length(x))
count <- 1
for (val in x) {
  hboost <- gbm(
    dep_delay ~ .,
    data = lm_train_df,
    n.trees = val,
    distribution = 'gaussian',
    shrinkage = 0.01
  )
  preds = predict(hboost, n.trees = val, newdata = lm_test_df)
  mse = mean((lm_test_df$dep_delay - preds) ^ 2)
  mse
  sqrt(mse)
  mse_vec[count] <- mse
 
  print(val)
  print(mse)
  count = count + 1
}

plot(x, mse_vec)
```


# it model where mse is a function of number of trees and shrinkage to predict optimal hyperparamter that balances performance and computational intensity.



```{r}
library(Matrix)
cat(rankMatrix(lm_train_df), "\n")   #prints 4
cat(rankMatrix(lm_test_df), "\n")    #prints 3
```



# using gbm
```{r}
hboost <- gbm(dep_delay ~ ., data=train_df,
              n.trees=1000, shrinkage=0.01)
```

```{r}
preds = predict(hboost, n.trees = 1000, newdata=test_df)
mse = mean((test_df$dep_delay - preds)^2)
mse
sqrt(mse)

```

```{r}
help("flights")
```



```{r}
help(weather)
```

```{r}
class(data_df$carrier)
```




```{r}
hboost <- gbm(first_formula, data=data_df,
              n.trees=100, shrinkage=0.01) # default shrinkage = 0.1
summary(hboost)
```


```{r}
data_df[!is.na(data_df$dep_delay),]
```


```{r}
data_df %>% dplyr::mutate_all(as.factor) %>% str
```

```{r}
(l <- sapply(data_df, function(x) is.factor(x)))
```
 

```{r}
library(Hmisc)
describe(data_df)
```


# 4. Results
# 5. Conclusions and Discussion  

    
The reports will be judged on the following criteria.

* Content (20): 
The content should be clear, accurate, complete and at the 
level of students in Stat 452. In the Methods you should provide a brief
description of any statistical methods you use. Please
restrict yourself to methods that were covered
in class. You can mention methods not covered as
areas of future work. 
Methods you considered 
but were not the focus of your report should be briefly mentioned 
here too.
In Results you should summarize and interpret the fitted model. 
Though the primary goal is prediction, your insights into the 
data-generating process are important. Refer to the Appendix for
the code that implements your prediction equation.
In the Conclusions and Discussion present your conclusions, 
discuss short-comings of your approach, and, optionally,
ideas for further work.
* Organization (3): Though the report is structured, you should
present your ideas logically within each section.
* Grammar and spelling (2): Please proof-read your report.

### Code (15 marks)

The code in your Appendix
should look correct and be readable. 
Given the size of the dataset, you do
not need to provide run-able code for
all analyses. Use `{r, eval=FALSE}` in your computationally-intensive code chunks
to prevent them from running.
The Appendix will be judged on the following criteria.

* Software Details (2): List the version of R you are using
and the names of all packages used in your analysis **at the beginning**
of the Appendix. Please also provide an estimate of the time
it will take to knit the code if more than about 2 minutes.
* Correctness (5): There should be no errors in data
processing, function calls, etc.
* Readability (5): The steps of your analysis should 
be clearly layed out and it should be easy for the reader
to find the final prediction equation/method.
* Efficiency (3): Please take steps to avoid computational
inefficiencies, such as loops and excessive copying of 
large R objects.


Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.

## Appendix
Using R version 3.6.1 Action of the Toes
Time to run on my local machine takes less than 2 minutes.



---
title: "STAT 652: Predicting Flight Delays Project"
author: "Vincent Chiu"
date: "11/26/2019"
output: 
  bookdown::pdf_document2:
    toc: true
    fig_caption: yes
bibliography: bibliography.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Introduction
The goal of this project is to predict the response variable, departure delays for a particular flight given the explanatory variables. 


# Data
The dataset consists of information about all the flights leaving from New York City in 2013.
The dataset contains 43 variables in total.  The dataset is an amalgamation of several datasets including datasets containing information on weather, the airports, the flights, and the models of airplanes. 
The training dataset provided to us contains 200,000 observations.
Please see table \@ref(tab:table1) for what the data looks like. 


# Methods:
We will now outline the various methods used to clean and perform prediction on the data. 
We will discuss our techniques for data preprocessing, and cross validation, and the different models that we tried. 

## Data Preprocessing
I performed data preprocessing on the nycflights13 dataset. 
My data preprocessing steps include the following:

* loading the data from a csv
* setting the random seed for reproducibility of results 
* casting all the columns with the character data type into the factor data type
* converting the shed_arr_time and sched_dep_time columns into the POSIX time format so that I can accurately take the difference of them.
* Dropping columns that contain data from after the planes' departure which may leak information about the response variable dep_delay. We drop columns "dep_time", "arr_time", "air_time", and "arr_delay".
* We drop column  "year.x" because all the values are 2013
* We also drop tailnum because it produces too many dummy variable columns for one hot encoding. 
* Dropping columns which consists of over 50% NAs which include the speed column.  However, it should be noted that a rule of thumb suggested by Professor McNeney is to drop any columns with over 5% NAs.   We use a different threshold for dropping columns leading to us keeping columns such as model instead of dropping it.
* Afterwards, I impute the missing values. However, there are limitations to this approach of imputing the missing values.  It is possible, that the missingness of the plane model variable is related to dep_delay.  In this scenario, we may be creating an inferior feature set by keeping the variable 'model' and imputing it.  For example, say a highly unreliable plane model that frequently causes long delays has a high probability of being labeled as NA and represents the majority of NAs in the dataset. We would not be able to capture the relationship between this plane model and dep_delay if we imputed the model with the mode.  To counteract this affect, we imputed NAs for the remaining columns using the imputeMissings library, adding a Boolean flag which indicates 1 if the associated value was 1 and 0 otherwise. For example, the "model_flag" for a given row is 1 if the "model" value was NA for that given row.  Hence, no information is lost from our imputation. 
* Normalizing the data to work well with methods like lasso regression.
* Only kept data which had a departure delay of less than 30 minutes late, which reduced the dataset from 200,000 rows to approximately 170,000. This is because we consider extreme delays of of over 30 minutes late to be freak accidents which cannot be accurately predicted by the available explanatory variables. 


## Exploratory Analysis

### Correlations
First, we created a correlation plot for the numeric variables to see if there any correlations between the variables.

We see that there is very little correlation between the response variable dep_delay and any of the other variables. 
Some of the strongest correlations include the correlation between distance and longitude and time zone and a smaller correlation between distance and latitude. This makes sense as most of the planes are inter US flights from west to east or vice versa, there is not as much distance flown in the north south direction. 
Please see Figure \@ref(fig:corrplot) for the correlation plot (Click on the number after Figure to jump to plot).


## Principal Component Analysis (PCA)
Next we performed PCA on only the numeric variables as techniques to perform PCA on mixed datasets (numerical and categorical) was not covered in class. 
When looking at the contribution of each variable to the first principal component, we notice that the variables lon, distance, tz, seats, alt, sched_air_time have the greatest absolute coefficients for the first principal component.  The fact that the aforementioned variables have large coefficients in the first principal component suggests that they are highly correlated with each other.  The fact that dep_delay has a small coefficient in the first principal component suggests that dep_delay is not highly correlated with any of the above variables.

As expected, it turns out that variables like lon, distance and tz are not important for predicting dep_delay according to the gbm model.  This maybe be because although variables like lon, distance and tz help explain most of the variance in the dataset, they have a weak relationship with dep_delay. 

Please see table \@ref(tab:pve) for the proportion of variance explained by each principal component. 
Please see table \@ref(tab:pcarotation) for the coefficient of each variable for each principal component ordered by magnitude of coefficient. 


## Cross Validation 
Initially, I used the most basic cross validation technique where I have a training dataset and a cross validation dataset. I split the original data into a ratio of 2/3 train and 1/3 of the data for cross validation.  There is a additional data which would be provided by the professor at a later date which we will use as the holdout test set.  I believe that 2/3 of the data gives enough data for the models to train on while 1/3 is enough data for us to get an accurate assessment of the error. k-folds cross validation was not initially used in order to save on compute time as we were initially only exploring the models. k-folds cross validation would increase training time for the models by a factor of k.  However, k-folds cross validation would lead to a more stable estimate of holdout test set error. 


## Models
We first explored some basic models to establish a baseline performance and compared it to our most sophisticated model, the Generalized Boosted Regression Model (GBM).

### Basic Models
dep_delay is the number of minutes that the plane either departs early or late.  Negative numbers are for early departures and positive numbers are for the number of minutes the plane is late.
First, I used a basic model of simply predicting the dep_delay to always be 0. This was done to establish baseline performance.  
This model had an root mean squared error (RMSE) of 8.30571. TODO
The model in which I predicted the mean for all the predictions had an RMSE of    TODO. 

### Linear Regression
Then I tried linear regression with dep_delay as the response variables and all the other remaining variables as the explanatory variables.
This model was better than predicting the mean with an RMSE of TODO. 
This suggests that there is some relationship between the dep_delay and the explanatory variables. 
TODO explain linear regression more

### Generalized Boosted Regression Model (GBM)
  Afterwards, we tried a Generalized Boosted Regression Model (GBM).  This model had the lowest RMSE on the test dataset after it was tuned to have a shrinkage of 0.01 and around 16,000 trees.  Shrinkage is proportional to the learning rate. 16,000 trees is the number of trees used in the model.  Each iteration uses 1 tree, so 16,000 trees also refers to the number of iterations. According to the vignette, the RMSE can always be improved by decreasing shrinkage, but this provides diminishing returns.  A good strategy would be to pick a small shrinkage that balances performance and compute time.  Then with this fixed shrinkage value, increase the number of trees until you get diminishing returns.  We decided to follow the aforementioned strategy.
Please see table @\ref{tab:gbmsummary} for a table of the relative influence of each explanatory variable. 
  Here, you can see the relative influence for each variable for gbm.  
For a gbm, the improvement in the splitting criterion (which is mean squared error for regression) for a given variable is calculated at each step.
The relative influence for a given variable is the average of these improvements over all the trees where the aforementioned variable is used.


# Results
In regression and gbm, I found different features to be important.  

## GBM
For the best gbm model, dest which refers to which airport a given plane was flying to was the most important feature. However, the one hot encoding versions of carrier were the most important features for regression.  


## Linear Regression
On the other hand, dest does appear as an important feature in linear regression as well but it is not the most important feature.  I surmise that if we can somehow sum up all the contributions from each of the one-hot-encoded variables derived from dest then, it might appear as the most important feature for linear regression as well. We can try using ANOVA in order to measure the statistical significance of dest. Performing ANOVA on comparing linear regression model with and without dest, it was determined that due to the low p-value of 0.0001863 associated with having dest that keeping at least one of the one hot categorical variables derived from dest is beneficial for the linear regression model. 



TODO: try interaction terms , try anova. 


# Conclusion and Discussion 


## Discussion

We considered removing outliers in train but not in test, then use k-folds cross validation on test to determine how many outliers we should remove to boost performance on the cross-validation set. 
We considered removing highly influential points in order to train a better model. In this case, we consider highly influential points to be points with high cook's distances.
However, this was infeasible as we did not have enough computational resources available and it took too long. 




## Conclusion

In conclusion, out of the methods that we covered in class, I found gradient boosted models to provide the best performance based on having the lowest root mean squared error on the hold out test set.

Based on the relative influence scores provided by the gbm, some of the most important feature variables include dest, model, and sched_dep_time_num_minute.

The dest column contains the airport code for where a given flight is flying to.
Based on my run of gbm with a shrinkage of 0.01 and 16834 trees, dest was the most important feature with 49.56 relative influence. [@uc_influence].


## Future Work

TODO: remove points that are outliers ie dep_delay > 200 or 300 etc. or remove less than x number of points.
then use k-folds cross validation on cross validation set where no points were removed. 
can repeat k-folds for different seeds. 
can just try this on my quickest model, i.e. linear regression. 
should be bowl shape vs RMSE vs. number of points removed.  theoretically

I also considered removing based on cook's distance but this took too long to compute. 

5 folds with 10 different random seeds

have train, CV and test set
1/3 train, 1/3 CV, 1/3 test
2/3% train, 1/3%CV, wait for prof test set

try lasso regression


# Code


## Preparing the programming environment

```{r,echo=FALSE}
echo_flag <- FALSE
```

### Loading Libraries
```{r}
library(tidyverse)
```

## Data Preprocessing

### Loading the data
```{r}
library(nycflights13)
library(Hmisc)
set.seed(42)
original_data <- read_csv("fltrain.csv.gz")
DF <- original_data
```

```{r table1, echo=echo_flag}
knitr::kable(
  head(original_data), booktabs = TRUE,
  caption = 'A table of the first few rows of the nycflights13 data.'
)
```


turning all columns with datatype characters to factors. 
```{r}
DF[sapply(DF, is.character)] <- lapply(DF[sapply(DF, is.character)], 
                                       as.factor)
DF$flight <- as.factor(DF$flight)
```


```{r}
library(lubridate)
DF$sched_arr_time_posix <- as.POSIXct(str_pad(as.character(DF$sched_arr_time), 4, pad="0"),format="%H%M")
DF$sched_arr_time_hour <- hour(DF$sched_arr_time_posix)
DF$sched_arr_time_minute <- minute(DF$sched_arr_time_posix)

#num minute is number of minutes since start of day for scheduled arrival time
DF$sched_arr_time_num_minute <- 60*DF$sched_arr_time_hour + DF$sched_arr_time_minute

DF$sched_dep_time_posix <- as.POSIXct(str_pad(as.character(DF$sched_dep_time),4 , pad="0"),format="%H%M")
DF$sched_dep_time_hour <- hour(DF$sched_dep_time_posix)
DF$sched_dep_time_minute <- minute(DF$sched_dep_time_posix)
#num minute is number of minutes since start of day for scheduled depival time
DF$sched_dep_time_num_minute <- 60*DF$sched_dep_time_hour + DF$sched_dep_time_minute
```
```{r}
select(original_data, time_hour, sched_dep_time, sched_arr_time, tz, tzone)
select(DF, sched_arr_time, sched_arr_time_hour)
```

```{r}
DF$sched_air_time <- DF$sched_arr_time_posix - DF$sched_dep_time_posix
drops <- c('sched_arr_time_posix', 'sched_arr_time_hour', 'sched_dep_time_posix', 'sched_dep_time_hour', 'sched_dep_time', 'sched_arr_time', 'hour', 'time', 'minute', 'time_hour' )
DF <- DF[ , !(names(DF) %in% drops)]
```

```{r}
drops <- c("dep_time", "arr_time", "air_time", "arr_delay", "year.x", 'tailnum')
DF <- DF[ , !(names(DF) %in% drops)]
```

```{r}
## Remove columns with more than 50% NA
DF <- DF[, -which(colMeans(is.na(DF)) > 0.5)]
```

```{r}
DF$sched_air_time <- as.numeric(DF$sched_air_time)
library(imputeMissings)
impute_model <- imputeMissings::compute(DF, method="median/mode")
impute_model
DF <- impute(DF, object=impute_model, flag=TRUE)
DF <- DF[!duplicated(as.list(DF))]  #remove all redundant flag columns that are identical to each other. 
```

```{r}
numeric_only_df <- dplyr::select_if(DF, is.numeric)
library(corrplot)
```

```{r corrplot, fig.cap = "My caption", echo=echo_flag}
corrplot(cor(numeric_only_df), type = 'lower')
```


## try features scaling
```{r}
dep_delay_vec <- DF$dep_delay
DF$dep_delay <- NULL
head(DF)

library(dplyr)
DF <- DF %>% mutate_if(is.numeric, scale)
head(DF)
DF$dep_delay <- dep_delay_vec
```

## Exploratory Data Analysis

```{r}
numeric_DF <- dplyr::select_if(DF, is.numeric) %>%  scale()
```

```{r}
prcomp_res <- prcomp(numeric_DF)
sdev <- prcomp_res$sdev
sdev
```

### all four components at same time
proportion of variance explained by each component
```{r}
pve <- colSums(prcomp_res$x^2)/sum(numeric_DF^2)
```

```{r pve, echo=echo_flag}
knitr::kable(
  head(pve), booktabs = TRUE,
  caption = 'proportion of variance explained by each principal component'
)
```


```{r}
rotation <- as.data.frame(prcomp_res$rotation)
rotation[order(-abs(rotation$PC1)),]
```
```{r}
pca_rotation <- head(rotation[order(-abs(rotation$PC2)),])
```

```{r pcarotation, echo=echo_flag}
knitr::kable(
  head(pca_rotation), booktabs = TRUE,
  caption = 'coefficients for each variable on each principal component'
)
```

### take out extreme departure delays
```{r}
DF<-DF[DF$dep_delay < 30,]
```

```{r}
set.seed(42)
DF$flight <- NULL
train_index <- sample(1:nrow(DF),size=2*nrow(DF)/3,replace=FALSE)
train_df <- DF[train_index,]
test_df <- DF[-train_index,]
```


# predicting 0
```{r}
rmse = mean((test_df$dep_delay-0)^2) %>% sqrt()
rmse
```

# predicting the mean
```{r}
rmse = mean((test_df$dep_delay-mean(train_df$dep_delay))^2)%>% sqrt()
rmse
```


# predicting the median
```{r}
rmse = mean((test_df$dep_delay-median(train_df$dep_delay))^2)%>% sqrt()
rmse
```


# linear regression with dep
```{r}
model <- lm(dep_delay ~ ., data=train_df)
model_without_dep <-  lm(dep_delay ~ .-dest, data=train_df)
anova(model, model_without_dep)
summary <- round(summary(model)$coefficients,6)
sorteddf <- summary[order(summary[,ncol(summary)]),]
head(sorteddf)
```


```{r}
head(sorteddf)
```

```{r}
lm_test_df <- test_df

in_test_but_not_train <- setdiff(unique(lm_test_df$model), unique(train_df$model))
lm_test_df <- lm_test_df[ !lm_test_df$model %in% in_test_but_not_train, ]

in_test_but_not_train <- setdiff(unique(lm_test_df$dest), unique(train_df$dest))
lm_test_df <- lm_test_df[ !lm_test_df$dest %in% in_test_but_not_train, ]

preds = predict(model, newdata=lm_test_df)
rmse = sqrt(mean((lm_test_df$dep_delay - preds)^2))
rmse
```


# gbm
```{r}
library(gbm)

train_gbm <- function(filename){
set.seed(42)
model <- gbm(dep_delay ~ ., data=train_df,
              n.trees=100, shrinkage=0.1) # default shrinkage = 0.1
preds = predict(model, newdata=test_df, n.trees=100)
rmse = sqrt(mean((test_df$dep_delay - preds)^2))
summary(model)
saveRDS(model, filename)
return(model)
}

destfile <- "model2.rds"
if (!file.exists(destfile)) {
   train_gbm(destfile)
 }
model <- readRDS(destfile)

```


```{r gbmsummary, echo=echo_flag}
gbmsummary <- summary(model)
knitr::kable(
  head(gbmsummary), booktabs = TRUE,
  caption = 'gbm relative influence'
)
```

Here, you can see the relative influence for each variable for gbm.  
For a gbm, the improvement in the splitting criterion (which is mean squared error for regression) for a given variable is calculated at each step.
The relative influence for a given variable is the average of these improvements over all the trees where the aforementioned variable is used.



```{r}
model <- gbm(dep_delay ~ ., data=train_df,
              n.trees=1000, shrinkage=0.01) # default shrinkage = 0.1
preds = predict(model, newdata=test_df, n.trees=1000)
rmse = sqrt(mean((test_df$dep_delay - preds)^2))
rmse
summary(model)
```

```{r}
rmse = sqrt(mean((test_df$dep_delay - preds)^2))
rmse
```


# References




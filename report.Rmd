---
title: "Stat 652 Project"
author: "Vincent Chiu"
date: '2019-10-16'
output: pdf_document
bibliography: bibliography.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Loading Libraries
```{r}
library(tidyverse)
```



## Loading the data
```{r}
library(nycflights13)
library(Hmisc)
set.seed(42)
original_data <- read_csv("fltrain.csv.gz")
DF <- original_data
```

# turning all columns with datatype characters to factors. 
```{r}
DF[sapply(DF, is.character)] <- lapply(DF[sapply(DF, is.character)], 
                                       as.factor)
DF$flight <- as.factor(DF$flight)
str(DF)
```


# Methods 
## Preprocessing

Data preprocessing steps include the following:
- Dropping columns that contain data from after the planes' departure which may leak information about the response variable dep_delay.
- Dropping columns with too many NAs.
- Impute NAs for the remaining columns. 
- Scaling the data to work well with methods like lasso regression.


## - Dropping columns that contain data from after the planes' departure which may leak information about the response variable dep_delay.

dropping the columns "dep_time", "arr_time", "air_time", "arr_delay", because that leaks the response variable. 
dropping column  "year.x" because all the values are 2013
dropping tailnum because it produces too many dummy variable columns for one hot encoding. 

```{r}
library(lubridate)
DF$sched_arr_time_posix <- as.POSIXct(str_pad(as.character(DF$sched_arr_time), 4, pad="0"),format="%H%M")
DF$sched_arr_time_hour <- hour(DF$sched_arr_time_posix)
DF$sched_arr_time_minute <- minute(DF$sched_arr_time_posix)

#num minute is number of minutes since start of day for scheduled arrival time
DF$sched_arr_time_num_minute <- 60*DF$sched_arr_time_hour + DF$sched_arr_time_minute

DF$sched_dep_time_posix <- as.POSIXct(str_pad(as.character(DF$sched_dep_time),4 , pad="0"),format="%H%M")
DF$sched_dep_time_hour <- hour(DF$sched_dep_time_posix)
DF$sched_dep_time_minute <- minute(DF$sched_dep_time_posix)
#num minute is number of minutes since start of day for scheduled depival time
DF$sched_dep_time_num_minute <- 60*DF$sched_dep_time_hour + DF$sched_dep_time_minute
```
```{r}
select(original_data, time_hour, sched_dep_time, sched_arr_time, tz, tzone)
select(DF, sched_arr_time, sched_arr_time_hour)
```

```{r}
DF$sched_air_time <- DF$sched_arr_time_posix - DF$sched_dep_time_posix
drops <- c('sched_arr_time_posix', 'sched_arr_time_hour', 'sched_dep_time_posix', 'sched_dep_time_hour', 'sched_dep_time', 'sched_arr_time', 'hour', 'time', 'minute', 'time_hour' )
DF <- DF[ , !(names(DF) %in% drops)]
```


```{r}
drops <- c("dep_time", "arr_time", "air_time", "arr_delay", "year.x", 'tailnum')
DF <- DF[ , !(names(DF) %in% drops)]
```



```{r}
DF
```

```{r}
## Remove columns with more than 50% NA
DF <- DF[, -which(colMeans(is.na(DF)) > 0.5)]
```

```{r}
DF$sched_air_time <- as.numeric(DF$sched_air_time)
library(imputeMissings)
impute_model <- imputeMissings::compute(DF, method="median/mode")
impute_model
DF <- impute(DF, object=impute_model, , flag=TRUE)
```

```{r}
numeric_only_df <- dplyr::select_if(DF, is.numeric)
library(corrplot)
corrplot(cor(numeric_only_df), type = 'lower')
```

# try features scaling
```{r}
dep_delay_vec <- DF$dep_delay
DF$dep_delay <- NULL
head(DF)

library(dplyr)
DF <- DF %>% mutate_if(is.numeric, scale)
head(DF)
DF$dep_delay <- dep_delay_vec
```

#take out extreme departure delays
```{r}
DF<-DF[DF$dep_delay < 30,]
```

```{r}
set.seed(42)
DF$flight <- NULL
train_index <- sample(1:nrow(DF),size=2*nrow(DF)/3,replace=FALSE)
train_df <- DF[train_index,]
test_df <- DF[-train_index,]
```


# predicting 0
```{r}
rmse = mean((test_df$dep_delay-0)^2) %>% sqrt()
rmse
```

# predicting the mean
```{r}
rmse = mean((test_df$dep_delay-mean(train_df$dep_delay))^2)%>% sqrt()
rmse
```


# predicting the median
```{r}
rmse = mean((test_df$dep_delay-median(train_df$dep_delay))^2)%>% sqrt()
rmse
```


# linear regression
```{r}
model <- lm(dep_delay ~ .-model, data=train_df)

summary <- round(summary(model)$coefficients,6)
sorteddf <- summary[order(summary[,ncol(summary)]),]
head(sorteddf)
```
```{r}
sorteddf
```

```{r}
lm_test_df <- test_df

in_test_but_not_train <- setdiff(unique(lm_test_df$model), unique(train_df$model))
lm_test_df <- lm_test_df[ !lm_test_df$model %in% in_test_but_not_train, ]

in_test_but_not_train <- setdiff(unique(lm_test_df$dest), unique(train_df$dest))
lm_test_df <- lm_test_df[ !lm_test_df$dest %in% in_test_but_not_train, ]

preds = predict(model, newdata=lm_test_df)
rmse = sqrt(mean((lm_test_df$dep_delay - preds)^2))
rmse
```

# gbm
```{r}
set.seed(42)
library(gbm)
model <- gbm(dep_delay ~ ., data=train_df,
              n.trees=100, shrinkage=0.1, cv.folds = 3) # default shrinkage = 0.1
preds = predict(model, newdata=test_df, n.trees=100)
rmse = sqrt(mean((test_df$dep_delay - preds)^2))
rmse
summary(model)
```

Here, you can see the relative influence for each variable for gbm.  
For a gbm, the improvement in the splitting criterion (which is mean squared error for regression) for a given variable is calculated at each step.
The relative influence for a given variable is the average of these improvements over all the trees where the aforementioned variable is used.



```{r}
model <- gbm(dep_delay ~ ., data=train_df,
              n.trees=1000, shrinkage=0.01) # default shrinkage = 0.1
preds = predict(model, newdata=test_df, n.trees=1000)
rmse = sqrt(mean((test_df$dep_delay - preds)^2))
rmse
summary(model)
```


set.seed(42)

x <- 2^seq(5,14, by=1)
rmse_vec <- numeric(length(x))
count <- 1
for (val in x) {
  hboost <- gbm(
    dep_delay ~ .,
    data = train_df,
    n.trees = val,
    distribution = 'gaussian',
    shrinkage = 0.01
  )
  preds = predict(hboost, n.trees = val, newdata = test_df)
  mse = mean((test_df$dep_delay - preds) ^ 2)
  rmse <- sqrt(mse)
  rmse_vec[count] <- rmse
 
  print(val)
  print(rmse)
  count = count + 1
}

plot(x, rmse_vec)




summary(hboost)
class(summary(hboost))
summary <- summary(hboost)
write.csv(summary,'16384trees_gbm.csv')




Analysis:

Tuning gbm
![alt text here](0point01_shrinkage_gbm_rmse_vs_num_trees.png)

Here I plotted root mean squared error (rmse) vs the number of trees for shrinkage of 0.01 and all other variables as default for gbm. 
You can see that after around 5000 trees, increasing the number of trees further gives diminishing returns.



#Methods:

## Data Preprocessing
I performed data preprocessing. 
My data preprocessing steps include the following:
- Dropping columns that contain data from after the planes' departure which may leak information about the response variable dep_delay.
- Dropping columns with too many NAs.
- Impute NAs for the remaining columns. 
- Scaling the data to work well with methods like lasso regression.
- Only kept data which had a departure delay of less than 30 minutes late, which reduced the dataset from 200,000 rows to approximately 170,000. 

## Modelling

# Basic Models
dep_delay is the number of minutes that the plane either departs early or late.  Negative numbers are for early departures and positive numbers are for the number of minutes the plane is late.
First, I used a basic model of simply predicting the dep_delay to always be 0. This was done to establish baseline performance.  
This model had an root mean squared error (RMSE) of 8.30571.



# Linear Regression

# GBM

# Basic

Conclusion:
In conclusion, out of the methods that we covered in class, I found gradient boosted models to provide the best performance based on having the lowest root mean squared error on the hold out test set.

Based on the relative influence scores provided by the gbm, some of the most important feature variables include dest, model, and sched_dep_time_num_minute.

The dest column contains the airport code for where a given flight is flying to.
Based on my run of gbm with a shrinkage of 0.01 and 16834 trees, dest was the most important feature with 49.56 relative influence.[@uc_influence].




# References